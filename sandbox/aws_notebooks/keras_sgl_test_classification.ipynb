{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from afqinsight.datasets import make_classification, make_sparse_group_classification\n",
    "from keras_ssg_lasso import sgl\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold\n",
    "\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First try without sparse groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples=50\n",
    "n_features=20\n",
    "n_informative=4\n",
    "n_redundant=0\n",
    "n_repeated=0\n",
    "\n",
    "X, y, idx = make_classification(\n",
    "    n_samples=n_samples,\n",
    "    n_features=n_features,\n",
    "    n_informative=n_informative,\n",
    "    n_redundant=n_redundant,\n",
    "    n_repeated=n_repeated,\n",
    "    flip_y=0.00,\n",
    "    class_sep=10.0,\n",
    "    n_classes=2,\n",
    "    useful_indices=True,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_class = np.transpose(np.array([y, 1-y], dtype=np.int32))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, y_class, test_size=0.15, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classification_model(lambda_=0.1):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(2, input_dim=n_features, activation='softmax', kernel_regularizer=regularizers.l1(lambda_)))\n",
    "    adam = Adam(lr=0.05)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(\n",
    "    build_fn=create_classification_model,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "# batch_size = [32, 64]\n",
    "epochs = [100]\n",
    "lambdas = np.logspace(-4, 4, 20)\n",
    "param_grid = dict(epochs=epochs, lambda_=lambdas)\n",
    "grid = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result = grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid.best_estimator_.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=3, verbose=1, mode='auto')\n",
    "checkpointer = ModelCheckpoint(filepath=\"best_weights.hdf5\", verbose=0, save_best_only=True) # save best model\n",
    "\n",
    "best_model.fit(X,y_class,validation_data=(x_test,y_test),callbacks=[monitor, checkpointer],verbose=1,epochs=1000)\n",
    "best_model.load_weights('best_weights.hdf5') # load weights from best model\n",
    "pred = best_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_hat = best_model.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('##.   coef  important  > 10^-3')\n",
    "print('--- ------- --------- ---------')\n",
    "for i, (b, beta) in enumerate(zip(idx, beta_hat[:, 0])):\n",
    "    print('{:02d}. {:+6.4f}   {:5s}     {:5s}'.format(i, beta, str(b), str(abs(beta) > 1e-2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do it again for a sparse group classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples=50\n",
    "n_groups=10\n",
    "n_informative_groups=2\n",
    "n_features_per_group=20\n",
    "n_informative_per_group=5\n",
    "n_redundant_per_group=0\n",
    "n_repeated_per_group=0\n",
    "\n",
    "X, y, idx = make_sparse_group_classification(\n",
    "    n_samples=n_samples,\n",
    "    n_groups=n_groups,\n",
    "    n_informative_groups=n_informative_groups,\n",
    "    n_features_per_group=n_features_per_group,\n",
    "    n_informative_per_group=n_informative_per_group,\n",
    "    n_redundant_per_group=n_redundant_per_group,\n",
    "    n_repeated_per_group=n_repeated_per_group,\n",
    "    n_classes=2,\n",
    "    n_clusters_per_class=2,\n",
    "    flip_y=0.0,\n",
    "    class_sep=10.0,\n",
    "    shuffle=True,\n",
    "    useful_indices=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_class = np.transpose(np.array([y, 1-y], dtype=np.int32))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = np.concatenate([np.ones(n_features_per_group) * i for i in range(n_groups)])\n",
    "ind_sparse = np.ones_like(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classification_model(alpha=0.1, lambda_=0.1):\n",
    "    model = sgl.SSGL_LogisticRegression(\n",
    "        dim_input=n_groups*n_features_per_group, n_classes=2, groups=groups, indices_sparse=ind_sparse,\n",
    "        n_epochs=500, alpha=alpha, lambda_=lambda_, optimizer='adam',\n",
    "        validation_split=0.0, early_stopping_patience=0,\n",
    "        verbose=True\n",
    "    )    \n",
    "    return model.model\n",
    "\n",
    "model = KerasClassifier(\n",
    "    build_fn=create_classification_model,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "# batch_size = [32, 64]\n",
    "epochs = [50]\n",
    "alphas = np.array([0.05, 0.5, 0.95])\n",
    "lambdas = np.logspace(-4, 4, 20)\n",
    "param_grid = dict(epochs=epochs, alpha=alphas, lambda_=lambdas)\n",
    "cv_generator = RepeatedStratifiedKFold(n_splits=3, n_repeats=3)\n",
    "scoring = {'AUC': 'roc_auc', 'Accuracy': 'accuracy'}\n",
    "grid = GridSearchCV(estimator=model,\n",
    "                    param_grid=param_grid,\n",
    "                    cv=cv_generator,\n",
    "                    scoring=scoring,\n",
    "                    refit='AUC',\n",
    "                    n_jobs=-1,\n",
    "                    verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result = grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid.best_estimator_.model\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=3, verbose=1, mode='auto')\n",
    "checkpointer = ModelCheckpoint(filepath=\"best_weights.hdf5\", verbose=0, save_best_only=True) # save best model\n",
    "\n",
    "best_model.fit(X,y,validation_data=(x_test,y_test),callbacks=[monitor, checkpointer],verbose=1,epochs=1000)\n",
    "best_model.load_weights('best_weights.hdf5') # load weights from best model\n",
    "pred = best_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_hat = best_model.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('##.    coef  important  > 10^-2')\n",
    "print('---- ------- --------- ---------')\n",
    "for i, (b, beta) in enumerate(zip(idx, beta_hat[:, 0])):\n",
    "    print('{:03d}. {:+6.4f}   {:5s}     {:5s}'.format(i, beta, str(b), str(abs(beta) >= 1e-2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pred > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for yt, yp in zip(y_test, y_pred):\n",
    "    print(yt, yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
